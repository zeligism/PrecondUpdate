{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "676c92ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "from math import log2\n",
    "from itertools import cycle, product\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d5ea7c",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "83e9552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data logs root directory\n",
    "LOG_DIR = \"logs_alphabeta/logistic\"\n",
    "\n",
    "# The following should be the same as the one used in run_experiment.py\n",
    "DATASETS = (\"a9a\", \"w8a\", \"rcv1\", \"real-sim\",)\n",
    "OPTIMIZERS = (\"Adam\", \"SARAH\", \"L-SVRG\")\n",
    "T = 100  # Use 2xT used in run_experiment.py\n",
    "\n",
    "# These are the metrics collected in the data logs\n",
    "METRICS = (\"loss\", \"gradnorm\", \"error\")\n",
    "METRIC = \"error\"  # choose metric\n",
    "\n",
    "# These are aggregators for comparing multi-seed runs\n",
    "AGGS = (\"mean\", \"median\")\n",
    "AGG = \"mean\"  # choose aggregator\n",
    "\n",
    "# Downsample this number of effective passes by averaging them\n",
    "AVG_DOWNSAMPLE = 5\n",
    "\n",
    "# These are the logs columns: effective passes + metrics\n",
    "LOG_COLS = [\"ep\"] + list(METRICS)\n",
    "\n",
    "# These are the hyperparameters of interest\n",
    "ARG_COLS = [\"lr\", \"alpha\", \"beta2\", \"precond\"]\n",
    "\n",
    "# Plots will be generated for this hyperparams/args setting.\n",
    "# 'corrupt' should be the scale/suffix of the dataset as a string or 'none'.\n",
    "FILTER_ARGS = {\n",
    "    \"corrupt\": \"none\",\n",
    "    \"weight_decay\": 0,\n",
    "}\n",
    "\n",
    "# Ignore all runs containing 'any' of these hyperparams.\n",
    "IGNORE_ARGS = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df62d34",
   "metadata": {},
   "source": [
    "### Utility functions for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8bee91a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore(args_dict):\n",
    "    return any(args_dict[arg] in IGNORE_ARGS[arg]\n",
    "               for arg in IGNORE_ARGS.keys() if arg in args_dict)\n",
    "\n",
    "\n",
    "def loaddata(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def contain_dict(dict1, dict2):\n",
    "    return all(dict1[k] == v for k, v in dict2.items() if k in dict1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d40c48",
   "metadata": {},
   "source": [
    "# Gathering data and finding best hyperparameters for each (optimizer, dataset) combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "835d9d33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs_alphabeta/logistic/w8a/Adam(seed=3,BS=128,lr=0.0625,beta2=0.999).pkl has no data!\n",
      "logs_alphabeta/logistic/w8a/Adam(seed=3,BS=128,lr=0.015625,beta2=0.999).pkl has no data!\n",
      "logs_alphabeta/logistic/w8a/Adam(seed=3,BS=128,lr=0.25,beta2=0.999).pkl has no data!\n",
      "logs_alphabeta/logistic/w8a/Adam(seed=3,BS=128,lr=4,beta2=0.999).pkl has no data!\n",
      "logs_alphabeta/logistic/w8a/Adam(seed=3,BS=128,lr=0.00390625,beta2=0.999).pkl has no data!\n",
      "logs_alphabeta/logistic/w8a/Adam(seed=3,BS=128,lr=1,beta2=0.999).pkl has no data!\n",
      "logs_alphabeta/logistic/w8a/Adam(seed=3,BS=128,lr=16,beta2=0.999).pkl has no data!\n"
     ]
    }
   ],
   "source": [
    "def unpack_args(fname):\n",
    "    \"\"\"\n",
    "    Recover all args given file path.\n",
    "    \"\"\"\n",
    "    args = {}\n",
    "    # unpack path\n",
    "    dirname, logname = os.path.split(fname)\n",
    "    logdir, args[\"dataset\"] = os.path.split(dirname)\n",
    "    # parse args\n",
    "    args[\"optimizer\"], argstr = logname.split(\"(\")\n",
    "    argstr, _ = argstr.split(\")\")  # remove ').pkl'\n",
    "    args_dict = {k:v for k,v in [s.split(\"=\") for s in argstr.split(\",\")]}\n",
    "\n",
    "    # Extract args\n",
    "    if args[\"dataset\"][-1] == \")\":\n",
    "        args[\"corrupt\"] = args[\"dataset\"][args[\"dataset\"].index(\"(\"):]\n",
    "    else:\n",
    "        # It is very unlikely that the original dataset name will end with ')'\n",
    "        args[\"corrupt\"] = \"none\"\n",
    "\n",
    "    if \"seed\" in args_dict:\n",
    "        args[\"seed\"] = int(args_dict[\"seed\"])\n",
    "    else:\n",
    "        args[\"seed\"] = 0\n",
    "\n",
    "    args[\"BS\"] = int(args_dict[\"BS\"])\n",
    "    args[\"lr\"] = float(args_dict[\"lr\"])\n",
    "    if \"weight_decay\" in args_dict:\n",
    "        args[\"weight_decay\"] = float(args_dict[\"weight_decay\"])\n",
    "    else:\n",
    "        args[\"weight_decay\"] = 0\n",
    "    if \"lr_decay\" in args_dict:\n",
    "        args[\"lr_decay\"] = float(args_dict[\"lr_decay\"])\n",
    "    else:\n",
    "        args[\"lr_decay\"] = 0\n",
    "    if \"p\" in args_dict:\n",
    "        args[\"p\"] = args_dict[\"p\"]\n",
    "    if \"precond\" in args_dict:\n",
    "        args[\"precond\"] = args_dict[\"precond\"]\n",
    "        args[\"beta2\"] = float(args_dict[\"beta2\"])\n",
    "        args[\"alpha\"] = float(args_dict[\"alpha\"])\n",
    "    else:\n",
    "        args[\"precond\"] = \"none\"\n",
    "        args[\"alpha\"] = \"none\"\n",
    "        args[\"beta2\"] = \"none\"\n",
    "    \n",
    "    if args[\"optimizer\"] == \"Adam\":\n",
    "        args[\"beta2\"] = float(args_dict[\"beta2\"])\n",
    "        args[\"alpha\"] = 1e-8\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_logs(logdir, dataset, optimizer, **filter_args):\n",
    "    \"\"\"\n",
    "    Return all logs in 'logdir' containing the filter hyperparams.\n",
    "    Dataset name should contain feature scaling, if any\n",
    "    e.g. 'dataset' or 'dataset(k_min,k_max)'.\n",
    "    \n",
    "    Returns the data in the log file and its arguments/hyperparams.\n",
    "    \"\"\"\n",
    "    remove_empty_data = False\n",
    "    # Add\n",
    "    if \"corrupt\" in filter_args and filter_args['corrupt'] != \"none\":\n",
    "        # Add scale suffix to specify dataset    \n",
    "        dataset += filter_args['corrupt']\n",
    "    else:\n",
    "        # No setting specified, use wildcard to match all suffixes\n",
    "        dataset += \"*\"\n",
    "    # Find all files matching this pattern\n",
    "    for fname in glob.glob(f\"{logdir}/{dataset}/{optimizer}(*).pkl\"):\n",
    "        exp_args = unpack_args(fname)\n",
    "        # Skip if filter_args do not match args of this file\n",
    "        if not contain_dict(exp_args, filter_args):\n",
    "            continue\n",
    "        # Load data\n",
    "        data = loaddata(fname)\n",
    "        # Handle empty data files\n",
    "        if len(data) == 0:\n",
    "            print(fname, \"has no data!\")\n",
    "            \"\"\"\n",
    "            if \"y\" == input(\"Remove empty log files in the future? (y/n)\"):\n",
    "                remove_empty_data = True\n",
    "            if remove_empty_data:\n",
    "                try:\n",
    "                    print(\"Removing\", fname)\n",
    "                    os.remove(fname)\n",
    "                except OSError as e:\n",
    "                    print (\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "            \"\"\"\n",
    "            continue\n",
    "        # @XXX: hack to correct wrong initial ep>0 for L-SVRG\n",
    "        ep0 = data[0,0]\n",
    "        if ep0 > 0.:\n",
    "            data[:,0] -= ep0\n",
    "        yield data, exp_args\n",
    "\n",
    "        \n",
    "# Gather data\n",
    "all_dfs = {}\n",
    "start_time = time.time()\n",
    "for exp in product(DATASETS, OPTIMIZERS):\n",
    "    exp_df = pd.DataFrame()\n",
    "    # Get all log data given the experiment and filter args\n",
    "    for data, args in get_logs(LOG_DIR, *exp, **FILTER_ARGS):\n",
    "        if ignore(args):\n",
    "            continue\n",
    "        # Get experiment log data\n",
    "        df = pd.DataFrame(data[:, :4], columns=LOG_COLS)\n",
    "        # Get args of interest\n",
    "        for col in ARG_COLS:\n",
    "            df[col] = args[col]\n",
    "        # Downsample by averaging metrics every AVG_DOWNSAMPLE epoch.\n",
    "        df[\"ep\"] = np.ceil(df[\"ep\"] / AVG_DOWNSAMPLE) * AVG_DOWNSAMPLE\n",
    "        df = df.groupby([\"ep\"] + ARG_COLS).mean().reset_index()\n",
    "        # Get data up to the prespecified epoch T\n",
    "        df = df[df[\"ep\"] <= T]\n",
    "        # @TODO: is this efficient?\n",
    "        exp_df = exp_df.append(df, ignore_index=True)\n",
    "    # Record all runs of exp in a single dataframe\n",
    "    all_dfs[exp] = exp_df\n",
    "\n",
    "    if len(exp_df) == 0:\n",
    "        print(\"No log data found for this experiment!\")\n",
    "        print(\"- Experiment:\", exp)\n",
    "        print(\"- filter_args:\", FILTER_ARGS)\n",
    "        continue\n",
    "data_gather_time = time.time() - start_time\n",
    "print(f\"Data frame lengths:\")\n",
    "for exp, df in all_dfs.items():\n",
    "    print(f\"{exp} -> {len(df)} data rows -> {len(df) // T} runs\")\n",
    "print(f\"Took about {data_gather_time:.2f} seconds to gather all these data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "81496a9e-0aa2-4969-a7b2-21081c7e3f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a9a', 'Adam')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ep</th>\n",
       "      <th>lr</th>\n",
       "      <th>alpha</th>\n",
       "      <th>beta2</th>\n",
       "      <th>precond</th>\n",
       "      <th>loss</th>\n",
       "      <th>gradnorm</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.999</td>\n",
       "      <td>none</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.032853</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.999</td>\n",
       "      <td>none</td>\n",
       "      <td>0.340162</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.156502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.999</td>\n",
       "      <td>none</td>\n",
       "      <td>0.326467</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.152164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.999</td>\n",
       "      <td>none</td>\n",
       "      <td>0.325896</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.151791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.999</td>\n",
       "      <td>none</td>\n",
       "      <td>0.326283</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.152156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>80.0</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.999</td>\n",
       "      <td>none</td>\n",
       "      <td>0.543354</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>0.186708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>85.0</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.999</td>\n",
       "      <td>none</td>\n",
       "      <td>0.542057</td>\n",
       "      <td>0.003305</td>\n",
       "      <td>0.194414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>90.0</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.999</td>\n",
       "      <td>none</td>\n",
       "      <td>0.591068</td>\n",
       "      <td>0.004067</td>\n",
       "      <td>0.194331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3001</th>\n",
       "      <td>95.0</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.999</td>\n",
       "      <td>none</td>\n",
       "      <td>0.559465</td>\n",
       "      <td>0.003294</td>\n",
       "      <td>0.191398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3002</th>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.999</td>\n",
       "      <td>none</td>\n",
       "      <td>0.582861</td>\n",
       "      <td>0.003920</td>\n",
       "      <td>0.191958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3003 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ep      lr         alpha  beta2 precond      loss  gradnorm     error\n",
       "0       0.0  0.0625  1.000000e-08  0.999    none  0.693147  0.032853  0.500000\n",
       "1       5.0  0.0625  1.000000e-08  0.999    none  0.340162  0.000939  0.156502\n",
       "2      10.0  0.0625  1.000000e-08  0.999    none  0.326467  0.000172  0.152164\n",
       "3      15.0  0.0625  1.000000e-08  0.999    none  0.325896  0.000108  0.151791\n",
       "4      20.0  0.0625  1.000000e-08  0.999    none  0.326283  0.000157  0.152156\n",
       "...     ...     ...           ...    ...     ...       ...       ...       ...\n",
       "2998   80.0  4.0000  1.000000e-08  0.999    none  0.543354  0.003034  0.186708\n",
       "2999   85.0  4.0000  1.000000e-08  0.999    none  0.542057  0.003305  0.194414\n",
       "3000   90.0  4.0000  1.000000e-08  0.999    none  0.591068  0.004067  0.194331\n",
       "3001   95.0  4.0000  1.000000e-08  0.999    none  0.559465  0.003294  0.191398\n",
       "3002  100.0  4.0000  1.000000e-08  0.999    none  0.582861  0.003920  0.191958\n",
       "\n",
       "[3003 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a9a', 'SARAH')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ep</th>\n",
       "      <th>lr</th>\n",
       "      <th>alpha</th>\n",
       "      <th>beta2</th>\n",
       "      <th>precond</th>\n",
       "      <th>loss</th>\n",
       "      <th>gradnorm</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.995</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.032853</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.995</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>0.826648</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.162838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.995</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>2.453245</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.152783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.995</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>47.543183</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.166875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.995</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>55.804485</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.155937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27688</th>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.995</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>41808.893660</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.195280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27689</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.995</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>32915.513302</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.212634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27690</th>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.995</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>36286.816012</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>0.218956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27691</th>\n",
       "      <td>95.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.995</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>33416.226438</td>\n",
       "      <td>0.001622</td>\n",
       "      <td>0.209002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27692</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.995</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>39705.204708</td>\n",
       "      <td>0.002385</td>\n",
       "      <td>0.253877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27693 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ep      lr         alpha  beta2     precond          loss  gradnorm  \\\n",
       "0        0.0  0.0625  1.000000e-07  0.995  hutchinson      0.693147  0.032853   \n",
       "1        5.0  0.0625  1.000000e-07  0.995  hutchinson      0.826648  0.000253   \n",
       "2       10.0  0.0625  1.000000e-07  0.995  hutchinson      2.453245  0.000003   \n",
       "3       15.0  0.0625  1.000000e-07  0.995  hutchinson     47.543183  0.000065   \n",
       "4       20.0  0.0625  1.000000e-07  0.995  hutchinson     55.804485  0.000005   \n",
       "...      ...     ...           ...    ...         ...           ...       ...   \n",
       "27688   80.0  0.0625  1.000000e-07  0.995  hutchinson  41808.893660  0.001817   \n",
       "27689   85.0  0.0625  1.000000e-07  0.995  hutchinson  32915.513302  0.001698   \n",
       "27690   90.0  0.0625  1.000000e-07  0.995  hutchinson  36286.816012  0.002485   \n",
       "27691   95.0  0.0625  1.000000e-07  0.995  hutchinson  33416.226438  0.001622   \n",
       "27692  100.0  0.0625  1.000000e-07  0.995  hutchinson  39705.204708  0.002385   \n",
       "\n",
       "          error  \n",
       "0      0.500000  \n",
       "1      0.162838  \n",
       "2      0.152783  \n",
       "3      0.166875  \n",
       "4      0.155937  \n",
       "...         ...  \n",
       "27688  0.195280  \n",
       "27689  0.212634  \n",
       "27690  0.218956  \n",
       "27691  0.209002  \n",
       "27692  0.253877  \n",
       "\n",
       "[27693 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a9a', 'L-SVRG')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ep</th>\n",
       "      <th>lr</th>\n",
       "      <th>alpha</th>\n",
       "      <th>beta2</th>\n",
       "      <th>precond</th>\n",
       "      <th>loss</th>\n",
       "      <th>gradnorm</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.95</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>6.931472e-01</td>\n",
       "      <td>3.285310e-02</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.95</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>3.183355e+05</td>\n",
       "      <td>1.905281e-02</td>\n",
       "      <td>0.235070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.95</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>1.496375e+06</td>\n",
       "      <td>1.728026e-02</td>\n",
       "      <td>0.208695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.95</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>1.240142e+06</td>\n",
       "      <td>1.450116e-02</td>\n",
       "      <td>0.194107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.0</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.95</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>1.429925e+06</td>\n",
       "      <td>1.801447e-02</td>\n",
       "      <td>0.216256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27715</th>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>3.266063e-01</td>\n",
       "      <td>1.116324e-06</td>\n",
       "      <td>0.153082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27716</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>3.263014e-01</td>\n",
       "      <td>9.412707e-07</td>\n",
       "      <td>0.153152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27717</th>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>3.261063e-01</td>\n",
       "      <td>8.351229e-07</td>\n",
       "      <td>0.153073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27718</th>\n",
       "      <td>95.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>3.259120e-01</td>\n",
       "      <td>7.346767e-07</td>\n",
       "      <td>0.153203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27719</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>3.257686e-01</td>\n",
       "      <td>6.631024e-07</td>\n",
       "      <td>0.153189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27720 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ep       lr         alpha  beta2     precond          loss  \\\n",
       "0        0.0  16.0000  1.000000e-07   0.95  hutchinson  6.931472e-01   \n",
       "1        5.0  16.0000  1.000000e-07   0.95  hutchinson  3.183355e+05   \n",
       "2       10.0  16.0000  1.000000e-07   0.95  hutchinson  1.496375e+06   \n",
       "3       15.0  16.0000  1.000000e-07   0.95  hutchinson  1.240142e+06   \n",
       "4       20.0  16.0000  1.000000e-07   0.95  hutchinson  1.429925e+06   \n",
       "...      ...      ...           ...    ...         ...           ...   \n",
       "27715   80.0   0.0625  1.000000e-01   0.99  hutchinson  3.266063e-01   \n",
       "27716   85.0   0.0625  1.000000e-01   0.99  hutchinson  3.263014e-01   \n",
       "27717   90.0   0.0625  1.000000e-01   0.99  hutchinson  3.261063e-01   \n",
       "27718   95.0   0.0625  1.000000e-01   0.99  hutchinson  3.259120e-01   \n",
       "27719  100.0   0.0625  1.000000e-01   0.99  hutchinson  3.257686e-01   \n",
       "\n",
       "           gradnorm     error  \n",
       "0      3.285310e-02  0.500000  \n",
       "1      1.905281e-02  0.235070  \n",
       "2      1.728026e-02  0.208695  \n",
       "3      1.450116e-02  0.194107  \n",
       "4      1.801447e-02  0.216256  \n",
       "...             ...       ...  \n",
       "27715  1.116324e-06  0.153082  \n",
       "27716  9.412707e-07  0.153152  \n",
       "27717  8.351229e-07  0.153073  \n",
       "27718  7.346767e-07  0.153203  \n",
       "27719  6.631024e-07  0.153189  \n",
       "\n",
       "[27720 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, (exp, df) in enumerate(all_dfs.items()):\n",
    "    if i == 3: break\n",
    "    print(exp)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd98f3f7-60e4-403b-92b8-795d8067e530",
   "metadata": {},
   "source": [
    "## Get best hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ca59c937-0832-4f7f-a48e-2e39b74d05a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best hyperparams for ('a9a', 'Adam')\n",
      "Finding best hyperparams for ('a9a', 'SARAH')\n",
      "Finding best hyperparams for ('a9a', 'L-SVRG')\n",
      "Finding best hyperparams for ('w8a', 'Adam')\n",
      "Finding best hyperparams for ('w8a', 'SARAH')\n",
      "Finding best hyperparams for ('w8a', 'L-SVRG')\n",
      "Finding best hyperparams for ('rcv1', 'Adam')\n",
      "Finding best hyperparams for ('rcv1', 'SARAH')\n",
      "Finding best hyperparams for ('rcv1', 'L-SVRG')\n",
      "Finding best hyperparams for ('real-sim', 'Adam')\n",
      "Finding best hyperparams for ('real-sim', 'SARAH')\n",
      "Finding best hyperparams for ('real-sim', 'L-SVRG')\n"
     ]
    }
   ],
   "source": [
    "for exp, df in all_dfs.items():\n",
    "    if exp[1] == \"Adam\": continue\n",
    "    alphas = set([] if \"alpha\" not in ARG_COLS else df[\"alpha\"])\n",
    "    betas = set([] if \"beta2\" not in ARG_COLS else df[\"beta2\"])\n",
    "    lrs = set([] if \"lr\" not in ARG_COLS else df[\"lr\"])\n",
    "    preconds = set([] if \"precond\" not in ARG_COLS else df[\"precond\"])\n",
    "    break\n",
    "\n",
    "best_dfs = {}\n",
    "best_dfs_alpha = {}\n",
    "best_dfs_beta = {}\n",
    "best_dfs_lr = {}\n",
    "best_dfs_precond = {}\n",
    "for exp in product(DATASETS, OPTIMIZERS):\n",
    "    print(\"Finding best hyperparams for\", exp)\n",
    "    best_dfs_alpha[exp] = {}\n",
    "    best_dfs_beta[exp] = {}\n",
    "    best_dfs_lr[exp] = {}\n",
    "    best_dfs_precond[exp] = {}\n",
    "    exp_df = all_dfs[exp]\n",
    "    # Get last metrics/performance (supposed to be epoch-smoothed for better results)\n",
    "    max_ep = exp_df.groupby(ARG_COLS, sort=False)[\"ep\"].transform(max)\n",
    "    perf = exp_df[exp_df[\"ep\"] == max_ep].drop(\"ep\", axis=1)\n",
    "    # Find the minimum aggregate metric (based on mean, median, etc.)\n",
    "    def find_best_hyperparams(perf):\n",
    "        if AGG == \"mean\":\n",
    "            agg_perf = perf.groupby(ARG_COLS).mean()\n",
    "        elif AGG == \"median\":\n",
    "            agg_perf = perf.groupby(ARG_COLS).median()\n",
    "        # Get the aggregated perf that minimizes the chosen metric\n",
    "        min_agg_perf = agg_perf[agg_perf[METRIC] == agg_perf.min()[METRIC]]\n",
    "        return min_agg_perf.index\n",
    "    # Get the data associated with the args of the min aggregated metric\n",
    "    exp_df = exp_df.set_index(ARG_COLS)\n",
    "    best_dfs[exp] = exp_df.loc[find_best_hyperparams(perf)]\n",
    "    for alpha in alphas:\n",
    "        best_dfs_alpha[exp][alpha] = exp_df.loc[find_best_hyperparams(perf[perf[\"alpha\"] == alpha])]\n",
    "    for beta in betas:\n",
    "        best_dfs_beta[exp][beta] = exp_df.loc[find_best_hyperparams(perf[perf[\"beta2\"] == beta])]\n",
    "    for lr in lrs:\n",
    "        best_dfs_lr[exp][lr] = exp_df.loc[find_best_hyperparams(perf[perf[\"lr\"] == lr])]\n",
    "    for precond in preconds:\n",
    "        best_dfs_precond[exp][precond] = exp_df.loc[find_best_hyperparams(perf[perf[\"precond\"] == precond])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "36d9874c-6a23-45c9-80f0-67bebd27b0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparams for each optimizer on each dataset given the following setting:\n",
      "{'corrupt': 'none', 'weight_decay': 0}\n",
      "\n",
      "('a9a', 'Adam')\n",
      "- lr = 2**-8\n",
      "- alpha = 1e-08\n",
      "- beta2 = 0.999\n",
      "- precond = none\n",
      "\n",
      "('a9a', 'SARAH')\n",
      "- lr = 2**-4\n",
      "- alpha = 0.001\n",
      "- beta2 = 0.995\n",
      "- precond = hutchinson\n",
      "\n",
      "('a9a', 'L-SVRG')\n",
      "- lr = 2**-4\n",
      "- alpha = 0.001\n",
      "- beta2 = 0.95\n",
      "- precond = hutchinson\n",
      "\n",
      "('w8a', 'Adam')\n",
      "- lr = 2**-6\n",
      "- alpha = 1e-08\n",
      "- beta2 = 0.999\n",
      "- precond = none\n",
      "\n",
      "('w8a', 'SARAH')\n",
      "- lr = 2**-10\n",
      "- alpha = 1e-07\n",
      "- beta2 = 0.999\n",
      "- precond = hutchinson\n",
      "\n",
      "('w8a', 'L-SVRG')\n",
      "- lr = 2**-8\n",
      "- alpha = 1e-07\n",
      "- beta2 = 0.999\n",
      "- precond = hutchinson\n",
      "\n",
      "('rcv1', 'Adam')\n",
      "- lr = 2**-6\n",
      "- alpha = 1e-08\n",
      "- beta2 = 0.995\n",
      "- precond = none\n",
      "\n",
      "('rcv1', 'SARAH')\n",
      "- lr = 2**-12\n",
      "- alpha = 1e-07\n",
      "- beta2 = 0.95\n",
      "- precond = hutchinson\n",
      "\n",
      "('rcv1', 'L-SVRG')\n",
      "- lr = 2**0\n",
      "- alpha = 0.001\n",
      "- beta2 = 0.95\n",
      "- precond = hutchinson\n",
      "\n",
      "('real-sim', 'Adam')\n",
      "- lr = 2**-6\n",
      "- alpha = 1e-08\n",
      "- beta2 = 0.999\n",
      "- precond = none\n",
      "\n",
      "('real-sim', 'SARAH')\n",
      "- lr = 2**-12\n",
      "- alpha = 1e-07\n",
      "- beta2 = 0.995\n",
      "- precond = hutchinson\n",
      "\n",
      "('real-sim', 'L-SVRG')\n",
      "- lr = 2**-10\n",
      "- alpha = 1e-07\n",
      "- beta2 = 0.99\n",
      "- precond = hutchinson\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparams for each optimizer on each dataset given the following setting:\")\n",
    "print(FILTER_ARGS)\n",
    "print()\n",
    "for exp, df in best_dfs.items():\n",
    "    print(exp)\n",
    "    for arg, val in zip(ARG_COLS, df.index[0]):\n",
    "        if arg == \"lr\":\n",
    "            val = \"2**\" + str(int(log2(val)))\n",
    "        print(f\"- {arg} = {val}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589bb07b",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9fd0ff",
   "metadata": {},
   "source": [
    "## Plot gradnorm per lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "161b091b-9c0a-48e0-9f37-7ab51ade0273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types\n",
      "ep float64\n",
      "loss float64\n",
      "gradnorm float64\n",
      "error float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Types\")\n",
    "for col in df.columns:\n",
    "    print(col, df[col].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b7f87599-2e6b-4030-b71e-2455347d9705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rates:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'2**-10',\n",
       " '2**-12',\n",
       " '2**-14',\n",
       " '2**-16',\n",
       " '2**-2',\n",
       " '2**-4',\n",
       " '2**-6',\n",
       " '2**-8',\n",
       " '2**0',\n",
       " '2**2',\n",
       " '2**4'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Learning rates:\")\n",
    "for exp, df in all_dfs.items():\n",
    "    display(set(\"2**\"+str(int(log2(lr))) for lr in df[\"lr\"]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "329af229-7514-43cf-a3b8-013508e623f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range\n",
      "ep: (0.0, 100.0)\n",
      "lr: (1.52587890625e-05, 16.0)\n",
      "alpha: (1e-08, 1e-08)\n",
      "beta2: (0.95, 0.999)\n",
      "loss: (0.32296919432635174, 2.204534029521757)\n",
      "gradnorm: (9.559924702890735e-07, 0.032853098105228115)\n",
      "error: (0.15025772140495275, 0.5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Range\")\n",
    "for col in df.columns:\n",
    "    if df[col].dtypes != \"object\":\n",
    "        print(f\"{col}: ({df[col].min():}, {df[col].max()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d2b7628b-1e89-482a-9cdc-055e14187188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: (0.0, 100.0)\n",
      "lr: (1.52587890625e-05, 16.0)\n",
      "alpha: (1e-08, 1e-08)\n",
      "beta2: (0.95, 0.999)\n",
      "loss: (0.32296919432635174, 2.204534029521757)\n",
      "gradnorm: (9.559924702890735e-07, 0.032853098105228115)\n",
      "error: (0.15025772140495275, 0.5)\n"
     ]
    }
   ],
   "source": [
    "# Set inf values to nan and recheck range\n",
    "VERYBIGNUMBER = 10**10\n",
    "df[df == float(\"inf\")] = np.nan\n",
    "df[df[[\"loss\",\"gradnorm\"]] > VERYBIGNUMBER] = np.nan\n",
    "for col in df.columns:\n",
    "    if df[col].dtypes != \"object\":\n",
    "        print(f\"{col}: ({df[col].min():}, {df[col].max()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "302e0f60-db61-4ed7-bf7f-7f38f554a0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for NaNs in each column for each df.\n",
      "ep\n",
      "- ('a9a', 'Adam'): 0\n",
      "- ('a9a', 'SARAH'): 0\n",
      "- ('a9a', 'L-SVRG'): 0\n",
      "- ('w8a', 'Adam'): 0\n",
      "- ('w8a', 'SARAH'): 0\n",
      "- ('w8a', 'L-SVRG'): 0\n",
      "- ('rcv1', 'Adam'): 0\n",
      "- ('rcv1', 'SARAH'): 0\n",
      "- ('rcv1', 'L-SVRG'): 0\n",
      "- ('real-sim', 'Adam'): 0\n",
      "- ('real-sim', 'SARAH'): 0\n",
      "- ('real-sim', 'L-SVRG'): 0\n",
      "lr\n",
      "- ('a9a', 'Adam'): 0\n",
      "- ('a9a', 'SARAH'): 0\n",
      "- ('a9a', 'L-SVRG'): 0\n",
      "- ('w8a', 'Adam'): 0\n",
      "- ('w8a', 'SARAH'): 0\n",
      "- ('w8a', 'L-SVRG'): 0\n",
      "- ('rcv1', 'Adam'): 0\n",
      "- ('rcv1', 'SARAH'): 0\n",
      "- ('rcv1', 'L-SVRG'): 0\n",
      "- ('real-sim', 'Adam'): 0\n",
      "- ('real-sim', 'SARAH'): 0\n",
      "- ('real-sim', 'L-SVRG'): 0\n",
      "alpha\n",
      "- ('a9a', 'Adam'): 0\n",
      "- ('a9a', 'SARAH'): 0\n",
      "- ('a9a', 'L-SVRG'): 0\n",
      "- ('w8a', 'Adam'): 0\n",
      "- ('w8a', 'SARAH'): 0\n",
      "- ('w8a', 'L-SVRG'): 0\n",
      "- ('rcv1', 'Adam'): 0\n",
      "- ('rcv1', 'SARAH'): 0\n",
      "- ('rcv1', 'L-SVRG'): 0\n",
      "- ('real-sim', 'Adam'): 0\n",
      "- ('real-sim', 'SARAH'): 0\n",
      "- ('real-sim', 'L-SVRG'): 0\n",
      "beta2\n",
      "- ('a9a', 'Adam'): 0\n",
      "- ('a9a', 'SARAH'): 0\n",
      "- ('a9a', 'L-SVRG'): 0\n",
      "- ('w8a', 'Adam'): 0\n",
      "- ('w8a', 'SARAH'): 0\n",
      "- ('w8a', 'L-SVRG'): 0\n",
      "- ('rcv1', 'Adam'): 0\n",
      "- ('rcv1', 'SARAH'): 0\n",
      "- ('rcv1', 'L-SVRG'): 0\n",
      "- ('real-sim', 'Adam'): 0\n",
      "- ('real-sim', 'SARAH'): 0\n",
      "- ('real-sim', 'L-SVRG'): 0\n",
      "precond\n",
      "- ('a9a', 'Adam'): 0\n",
      "- ('a9a', 'SARAH'): 0\n",
      "- ('a9a', 'L-SVRG'): 0\n",
      "- ('w8a', 'Adam'): 0\n",
      "- ('w8a', 'SARAH'): 0\n",
      "- ('w8a', 'L-SVRG'): 0\n",
      "- ('rcv1', 'Adam'): 0\n",
      "- ('rcv1', 'SARAH'): 0\n",
      "- ('rcv1', 'L-SVRG'): 0\n",
      "- ('real-sim', 'Adam'): 0\n",
      "- ('real-sim', 'SARAH'): 0\n",
      "- ('real-sim', 'L-SVRG'): 0\n",
      "loss\n",
      "- ('a9a', 'Adam'): 0\n",
      "- ('a9a', 'SARAH'): 0\n",
      "- ('a9a', 'L-SVRG'): 0\n",
      "- ('w8a', 'Adam'): 0\n",
      "- ('w8a', 'SARAH'): 0\n",
      "- ('w8a', 'L-SVRG'): 0\n",
      "- ('rcv1', 'Adam'): 0\n",
      "- ('rcv1', 'SARAH'): 0\n",
      "- ('rcv1', 'L-SVRG'): 0\n",
      "- ('real-sim', 'Adam'): 0\n",
      "- ('real-sim', 'SARAH'): 0\n",
      "- ('real-sim', 'L-SVRG'): 0\n",
      "gradnorm\n",
      "- ('a9a', 'Adam'): 0\n",
      "- ('a9a', 'SARAH'): 0\n",
      "- ('a9a', 'L-SVRG'): 0\n",
      "- ('w8a', 'Adam'): 0\n",
      "- ('w8a', 'SARAH'): 0\n",
      "- ('w8a', 'L-SVRG'): 0\n",
      "- ('rcv1', 'Adam'): 0\n",
      "- ('rcv1', 'SARAH'): 0\n",
      "- ('rcv1', 'L-SVRG'): 0\n",
      "- ('real-sim', 'Adam'): 0\n",
      "- ('real-sim', 'SARAH'): 0\n",
      "- ('real-sim', 'L-SVRG'): 0\n",
      "error\n",
      "- ('a9a', 'Adam'): 0\n",
      "- ('a9a', 'SARAH'): 0\n",
      "- ('a9a', 'L-SVRG'): 0\n",
      "- ('w8a', 'Adam'): 0\n",
      "- ('w8a', 'SARAH'): 0\n",
      "- ('w8a', 'L-SVRG'): 0\n",
      "- ('rcv1', 'Adam'): 0\n",
      "- ('rcv1', 'SARAH'): 0\n",
      "- ('rcv1', 'L-SVRG'): 0\n",
      "- ('real-sim', 'Adam'): 0\n",
      "- ('real-sim', 'SARAH'): 0\n",
      "- ('real-sim', 'L-SVRG'): 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Check for NaNs in each column for each df.\")\n",
    "for col in df.columns:\n",
    "    print(col)\n",
    "    for exp, df in all_dfs.items():\n",
    "        print(f\"- {exp}: {df[col].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "991aefe1-a97b-4faa-854b-28a03d95bf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting lines for ('a9a', 'Adam')...\n",
      "Plotting lines for ('w8a', 'Adam')...\n",
      "Plotting lines for ('rcv1', 'Adam')...\n",
      "Plotting lines for ('real-sim', 'Adam')...\n",
      "Plotting lines for ('a9a', 'SARAH')...\n",
      "Plotting lines for ('w8a', 'SARAH')...\n",
      "Plotting lines for ('rcv1', 'SARAH')...\n",
      "Plotting lines for ('real-sim', 'SARAH')...\n",
      "Plotting lines for ('a9a', 'L-SVRG')...\n",
      "Plotting lines for ('w8a', 'L-SVRG')...\n",
      "Plotting lines for ('rcv1', 'L-SVRG')...\n",
      "Plotting lines for ('real-sim', 'L-SVRG')...\n",
      "Took about 85.74 seconds to create this plot.\n"
     ]
    }
   ],
   "source": [
    "y = \"gradnorm\"  # METRICS\n",
    "mode = \"lrs\"  # (alphas, betas, lrs)\n",
    "\n",
    "best_dfs_mode = {\"alphas\": best_dfs_alpha, \"betas\": best_dfs_beta, \"lrs\": best_dfs_lr}\n",
    "mode_greek = {\"alphas\": r\"$\\alpha$\", \"betas\": r\"$\\beta$\", \"lrs\": r\"$\\eta$\"}\n",
    "y_greek = {\"loss\": r\"$P(w_t)$\", \"gradnorm\": r\"$||\\nabla P(w_t)||^2$\", \"error\": \"Error\"}\n",
    "valid_optimizers = [opt for opt in OPTIMIZERS if not (mode == \"alphas\" and opt == \"Adam\")]\n",
    "\n",
    "start_time = time.time()\n",
    "# Plot data for all optim, datasets, and args\n",
    "fig, axes = plt.subplots(len(valid_optimizers), len(DATASETS))\n",
    "fig.set_size_inches(5 * len(DATASETS), 5 * (len(valid_optimizers)))\n",
    "title = rf\"{y_greek[y]} per {mode_greek[mode]}\"\n",
    "plt.suptitle(title)\n",
    "for i, optimizer in enumerate(valid_optimizers):\n",
    "    for j, dataset in enumerate(DATASETS):\n",
    "        exp = (dataset, optimizer)\n",
    "        if exp not in best_dfs_lr:\n",
    "            continue\n",
    "        exp_df = pd.concat(list(best_dfs_mode[mode][exp].values())).reset_index()\n",
    "        if len(exp_df) == 0:\n",
    "            continue\n",
    "        axes[i,j].set_title(rf\"$\\tt {optimizer}({dataset})$\")\n",
    "        # avoid silly problem of inconsistent style across axes\n",
    "        exp_df[\"beta2\"] = exp_df[\"beta2\"].astype(str)\n",
    "        exp_df = exp_df.sort_values(\"alpha\", ascending=False)\n",
    "        exp_df = exp_df.sort_values(\"beta2\", ascending=True)\n",
    "        print(f\"Plotting lines for {exp}...\")\n",
    "        sns.lineplot(ax=axes[i,j], x=\"ep\", y=y,\n",
    "                     hue=\"lr\", hue_norm=LogNorm(), palette=\"vlag\",\n",
    "                     size=\"beta2\", style=\"alpha\", data=exp_df)\n",
    "        axes[i,j].set_ylabel(rf\"{y_greek[y]}\")\n",
    "        axes[i,j].set(yscale=\"log\")  # @TODO: always set log scale?\n",
    "fig.tight_layout()\n",
    "\n",
    "# Create a string out of filter args and save figure\n",
    "filter_args_str = \",\".join(f\"{k}={v}\" for k,v in FILTER_ARGS.items())\n",
    "plt.savefig(f\"plots/{mode}({filter_args_str}).pdf\")\n",
    "plt.close()\n",
    "plot_per_lr_time = time.time() - start_time\n",
    "print(f\"Took about {plot_per_lr_time:.2f} seconds to create this plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6130b8ea",
   "metadata": {},
   "source": [
    "## Plot best performance of each optimizer on each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "af103b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting lines for ('a9a', 'Adam')...\n",
      "Plotting lines for ('a9a', 'SARAH')...\n",
      "Plotting lines for ('a9a', 'L-SVRG')...\n",
      "Plotting lines for ('w8a', 'Adam')...\n",
      "Plotting lines for ('w8a', 'SARAH')...\n",
      "Plotting lines for ('w8a', 'L-SVRG')...\n",
      "Plotting lines for ('rcv1', 'Adam')...\n",
      "Plotting lines for ('rcv1', 'SARAH')...\n",
      "Plotting lines for ('rcv1', 'L-SVRG')...\n",
      "Plotting lines for ('real-sim', 'Adam')...\n",
      "Plotting lines for ('real-sim', 'SARAH')...\n",
      "Plotting lines for ('real-sim', 'L-SVRG')...\n",
      "Took about 20.31 seconds to create this plot.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Plot 3 rows each one showing some performance metric,\n",
    "# where the columns are the dataset on which the optim is run.\n",
    "fig, axes = plt.subplots(3, len(DATASETS))\n",
    "fig.set_size_inches(5 * len(DATASETS), 5 * 3)\n",
    "plt.suptitle(rf\"Best Performances\")\n",
    "for j, dataset in enumerate(DATASETS):\n",
    "    for optimizer in OPTIMIZERS:\n",
    "        exp = (dataset, optimizer)\n",
    "        if exp not in best_dfs:\n",
    "            continue\n",
    "        # Get hyperparams of best performance of 'optimizer' on 'dataset'\n",
    "        args = {k:v for k,v in zip(best_dfs[exp].index.names, best_dfs[exp].index[0])}\n",
    "        exp_df = best_dfs[exp].reset_index()\n",
    "        # Show power of lr as 2^lr_pow\n",
    "        lr_pow = round(log2(args['lr']))\n",
    "        if optimizer == \"Adam\":\n",
    "            sublabel = rf\"$\\eta = 2^{{{lr_pow}}}$, $\\beta_1=0.9$, $\\beta_2={args['beta2']}$\"\n",
    "        else:\n",
    "            sublabel = rf\"$\\eta = 2^{{{lr_pow}}}$, $\\alpha={args['alpha']}$, $\\beta={args['beta2']}$\"\n",
    "        label = rf\"{optimizer}({sublabel})\"\n",
    "        print(f\"Plotting lines for {exp}...\")\n",
    "        sns.lineplot(x=\"ep\", y=\"loss\", label=label, ax=axes[0,j], data=exp_df)\n",
    "        sns.lineplot(x=\"ep\", y=\"gradnorm\", label=label, ax=axes[1,j], data=exp_df)\n",
    "        sns.lineplot(x=\"ep\", y=\"error\", label=label, ax=axes[2,j], data=exp_df)\n",
    "    # Loss\n",
    "    axes[0,j].set_title(dataset)\n",
    "    axes[0,j].set_ylabel(r\"$P(w_t)$\")\n",
    "    axes[0,j].set_xlabel(\"Effective Passes\")\n",
    "    axes[0,j].legend()\n",
    "    # Gradnorm\n",
    "    axes[1,j].set(yscale=\"log\")\n",
    "    axes[1,j].set_title(dataset)\n",
    "    axes[1,j].set_ylabel(r\"$||\\nabla P(w_t)||^2$\")\n",
    "    axes[1,j].set_xlabel(\"Effective Passes\")\n",
    "    axes[1,j].legend()\n",
    "    # Error\n",
    "    axes[2,j].set(yscale=\"log\")\n",
    "    axes[2,j].set_title(dataset)\n",
    "    axes[2,j].set_ylabel(\"Error\")\n",
    "    axes[2,j].set_xlabel(\"Effective Passes\")\n",
    "    axes[2,j].legend()\n",
    "fig.tight_layout()\n",
    "\n",
    "# Create a string out of filter args and save figure\n",
    "filter_args_str = \",\".join(f\"{k}={v}\" for k,v in FILTER_ARGS.items())\n",
    "plt.savefig(f\"plots/overall({filter_args_str}).pdf\")\n",
    "plt.close()\n",
    "plot_best_time = time.time() - start_time\n",
    "print(f\"Took about {plot_best_time:.2f} seconds to create this plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737fe2bc",
   "metadata": {},
   "source": [
    "# Generate preconditioning comparison plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbbfce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "fig, axes = plt.subplots(3, len(DATASETS))\n",
    "fig.set_size_inches(5 * len(DATASETS), 5 * 3)\n",
    "plt.suptitle(rf\"Top performance beta\")\n",
    "for j, dataset in enumerate(DATASETS):\n",
    "    optim_df = pd.DataFrame()\n",
    "    for optimizer in OPTIMIZERS:\n",
    "        exp = (dataset, optimizer)\n",
    "        if exp not in best_dfs_precond:\n",
    "            continue\n",
    "        # Put dfs together and mark them with the optimizer's name.\n",
    "        exp_df = pd.concat(list(best_dfs_precond[exp].values()))\n",
    "        if len(exp_df) == 0:\n",
    "            continue\n",
    "        exp_df[\"optimizer\"] = optimizer\n",
    "        optim_df = optim_df.append(exp_df)\n",
    "    # reset index and combine precond with gamma\n",
    "    print(f\"Plotting lines for {dataset}...\")\n",
    "    optim_df = optim_df.reset_index()\n",
    "    sns.lineplot(x=\"ep\", y=\"loss\", hue=\"optimizer\", style=\"precond\", ax=axes[0,j], data=optim_df)\n",
    "    sns.lineplot(x=\"ep\", y=\"gradnorm\", hue=\"optimizer\", style=\"precond\", ax=axes[1,j], data=optim_df)\n",
    "    sns.lineplot(x=\"ep\", y=\"error\", hue=\"optimizer\", style=\"precond\", ax=axes[2,j], data=optim_df)\n",
    "    # Loss\n",
    "    axes[0,j].set_title(dataset)\n",
    "    axes[0,j].set_ylabel(r\"$P(w_t)$\")\n",
    "    axes[0,j].set_xlabel(\"Effective Passes\")\n",
    "    # Gradnorm\n",
    "    axes[1,j].set(yscale=\"log\")\n",
    "    axes[1,j].set_title(dataset)\n",
    "    axes[1,j].set_ylabel(r\"$||\\nabla P(w_t)||^2$\")\n",
    "    axes[1,j].set_xlabel(\"Effective Passes\")\n",
    "    # Error\n",
    "    axes[2,j].set(yscale=\"log\")\n",
    "    axes[2,j].set_title(dataset)\n",
    "    axes[2,j].set_ylabel(\"Error\")\n",
    "    axes[2,j].set_xlabel(\"Effective Passes\")\n",
    "fig.tight_layout()\n",
    "\n",
    "filter_args_str = \",\".join(f\"{k}={v}\" for k,v in FILTER_ARGS.items())\n",
    "plt.savefig(f\"plots/preconditioning({filter_args_str}).pdf\")\n",
    "plt.close()\n",
    "plot_compare_time = time.time() - start_time\n",
    "print(f\"Took about {plot_compare_time:.2f} seconds to create this plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fec6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_best_performances(best_data, show_alpha=True):\n",
    "    for dataset in DATASETS:\n",
    "        for optimizer in OPTIMIZERS:\n",
    "            # Extract best performance metrics for each experiment\n",
    "            exp = (dataset, optimizer)\n",
    "            if exp not in best_data:\n",
    "                continue\n",
    "            if len(best_data[exp].index) == 0:\n",
    "                # Not applicable to exp, likely because it does not have 'precond'\n",
    "                continue\n",
    "            args = {k:v for k,v in zip(best_data[exp].index.names, best_data[exp].index[0])}\n",
    "            exp_df = best_data[exp].reset_index()\n",
    "            loss = exp_df[\"loss\"].iloc[-1]\n",
    "            gradnorm = exp_df[\"gradnorm\"].iloc[-1]\n",
    "            error = exp_df[\"error\"].iloc[-1]\n",
    "            # Print report\n",
    "            print(f\"{exp}:\"\n",
    "                  f\"\\tlr = 2^{str(round(log2(args['lr'])))},\" + \\\n",
    "                  (f\"\\talpha = {args['alpha']},\" if show_alpha else \"\") + \\\n",
    "                  f\"\\tloss = {loss:5f},\"\n",
    "                  f\"\\tgradnorm = {gradnorm:5f},\"\n",
    "                  f\"\\terror = {error:5f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "print(f\"Best hyperparameters using {METRIC} metric WITHOUT preconditoning:\")\n",
    "display_best_performances(best_dfs_precond[\"none\"], show_alpha=False)\n",
    "print(f\"Best hyperparameters using {METRIC} metric WITH preconditoning:\")\n",
    "display_best_performances(best_dfs_precond[\"hutchinson\"], show_alpha=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85397467-886c-4ae1-9cbc-4f10f26b3ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
