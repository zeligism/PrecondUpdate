{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "676c92ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "from math import log2\n",
    "from itertools import cycle, product\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d5ea7c",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b736ec4-fb55-4ca9-b08b-c49356244398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data logs root directory\n",
    "LOG_DIR = \"logs_torch\"\n",
    "\n",
    "# Loss function: either \"logistic\" regression, or nonlinear least squares ('nllsq')\n",
    "LOSSES = (\"logistic\", \"nllsq\")\n",
    "LOSS = \"cross_entropy\"\n",
    "# Adjust logdir\n",
    "LOG_DIR = os.path.join(LOG_DIR, LOSS)\n",
    "\n",
    "# The following should be the same as the one used in run_experiment.py\n",
    "DATASETS = (\"mnist\",)\n",
    "dataset = \"mnist\"\n",
    "OPTIMIZERS = (\"L-SVRG\", \"Adam\")\n",
    "T = 25  # Use 2xT used in run_experiment.py\n",
    "\n",
    "# These are the metrics collected in the data logs\n",
    "METRICS = (\"loss\", \"gradnorm\", \"error\")\n",
    "METRIC = \"error\"  # choose metric\n",
    "\n",
    "# These are aggregators for comparing multi-seed runs\n",
    "AGGS = (\"mean\", \"median\")\n",
    "AGG = \"mean\"  # choose aggregator\n",
    "\n",
    "# Downsample this number of effective passes by averaging them\n",
    "AVG_DOWNSAMPLE = 2\n",
    "\n",
    "# These are the logs columns: effective passes + metrics\n",
    "LOG_COLS = [\"ep\"] + list(METRICS)\n",
    "\n",
    "# These are the hyperparameters of interest\n",
    "ARG_COLS = [\"lr\", \"alpha\", \"beta2\", \"precond\"]\n",
    "\n",
    "# Plots will be generated for this hyperparams/args setting.\n",
    "# 'corrupt' should be the scale/suffix of the dataset as a string or 'none'.\n",
    "FILTER_ARGS = {\n",
    "}\n",
    "SETTINGS_STR = f\"loss={LOSS},metric={METRIC},\" + \\\n",
    "               \",\".join(f\"{k}={v}\" for k,v in FILTER_ARGS.items())\n",
    "\n",
    "# Ignore all runs containing 'any' of these hyperparams.\n",
    "IGNORE_ARGS = {\n",
    "    \"alpha\": (1e-11,),\n",
    "    \"weight_decay\": (0.1,),\n",
    "}\n",
    "\n",
    "# Force remove log files that are empty\n",
    "FORCE_REMOVE_EMPTY_DATA = False\n",
    "\n",
    "# Aspect ratio and height of subplots\n",
    "ASPECT = 4. / 3.\n",
    "HEIGHT = 3.\n",
    "HEIGHT_LARGE = 4.\n",
    "LEGEND_FONTSIZE = \"x-small\"\n",
    "LEGEND_LOC = \"upper right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df62d34",
   "metadata": {},
   "source": [
    "### Utility functions for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8bee91a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore(args_dict):\n",
    "    return any(args_dict[arg] in map(str, IGNORE_ARGS[arg])\n",
    "               for arg in IGNORE_ARGS.keys() if arg in args_dict)\n",
    "\n",
    "\n",
    "def loaddata(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def contain_dict(dict1, dict2):\n",
    "    return all(dict1[k] == v for k, v in dict2.items() if k in dict1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d40c48",
   "metadata": {},
   "source": [
    "# Gathering data and finding best hyperparameters for each (optimizer, dataset) combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "835d9d33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs_torch/cross_entropy/mnist/Adam(seed=9,batch_size=128,lr=0.0009765625,beta1=0.9,beta2=0.999).pkl has no data!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Remove empty log files in the future without asking? (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will remove without asking.\n",
      "logs_torch/cross_entropy/mnist/Adam(seed=9,batch_size=128,lr=0.00390625,beta1=0.9,beta2=0.99).pkl has no data!\n",
      "Removing logs_torch/cross_entropy/mnist/Adam(seed=9,batch_size=128,lr=0.00390625,beta1=0.9,beta2=0.99).pkl\n",
      "logs_torch/cross_entropy/mnist/Adam(seed=9,batch_size=128,lr=0.015625,beta1=0.9,beta2=0.999).pkl has no data!\n",
      "Removing logs_torch/cross_entropy/mnist/Adam(seed=9,batch_size=128,lr=0.015625,beta1=0.9,beta2=0.999).pkl\n",
      "logs_torch/cross_entropy/mnist/Adam(seed=9,batch_size=128,lr=0.000244140625,beta1=0.9,beta2=0.99).pkl has no data!\n",
      "Removing logs_torch/cross_entropy/mnist/Adam(seed=9,batch_size=128,lr=0.000244140625,beta1=0.9,beta2=0.99).pkl\n",
      "logs_torch/cross_entropy/mnist/Adam(seed=9,batch_size=128,lr=0.015625,beta1=0.9,beta2=0.99).pkl has no data!\n",
      "Removing logs_torch/cross_entropy/mnist/Adam(seed=9,batch_size=128,lr=0.015625,beta1=0.9,beta2=0.99).pkl\n",
      "logs_torch/cross_entropy/mnist/Adam(seed=9,batch_size=128,lr=0.00390625,beta1=0.9,beta2=0.999).pkl has no data!\n",
      "Removing logs_torch/cross_entropy/mnist/Adam(seed=9,batch_size=128,lr=0.00390625,beta1=0.9,beta2=0.999).pkl\n",
      "logs_torch/cross_entropy/mnist/Adam(seed=9,batch_size=128,lr=0.000244140625,beta1=0.9,beta2=0.999).pkl has no data!\n",
      "Removing logs_torch/cross_entropy/mnist/Adam(seed=9,batch_size=128,lr=0.000244140625,beta1=0.9,beta2=0.999).pkl\n",
      "logs_torch/cross_entropy/mnist/Adam(seed=9,batch_size=128,lr=0.0009765625,beta1=0.9,beta2=0.99).pkl has no data!\n",
      "Removing logs_torch/cross_entropy/mnist/Adam(seed=9,batch_size=128,lr=0.0009765625,beta1=0.9,beta2=0.99).pkl\n",
      "Data frame lengths:\n",
      "('mnist', 'L-SVRG') -> 5131 data rows -> 205 runs\n",
      "('mnist', 'Adam') -> 1170 data rows -> 46 runs\n",
      "Took about 4.06 seconds to gather all these data.\n"
     ]
    }
   ],
   "source": [
    "REMOVE_EMPTY_DATA = False or FORCE_REMOVE_EMPTY_DATA\n",
    "\n",
    "\n",
    "def unpack_args(fname):\n",
    "    \"\"\"\n",
    "    Recover all args given file path.\n",
    "    \"\"\"\n",
    "    args = {}\n",
    "    # unpack path\n",
    "    dirname, logname = os.path.split(fname)\n",
    "    logdir, args[\"dataset\"] = os.path.split(dirname)\n",
    "    # parse args\n",
    "    args[\"optimizer\"], argstr = logname.split(\"(\")\n",
    "    argstr, _ = argstr.split(\")\")  # remove ').pkl'\n",
    "    args_dict = {k:v for k,v in [s.split(\"=\") for s in argstr.split(\",\")]}\n",
    "\n",
    "    # Extract args\n",
    "    if args[\"dataset\"][-1] == \")\":\n",
    "        args[\"corrupt\"] = args[\"dataset\"][args[\"dataset\"].index(\"(\"):]\n",
    "    else:\n",
    "        # It is very unlikely that the original dataset name will end with ')'\n",
    "        args[\"corrupt\"] = \"none\"\n",
    "\n",
    "    if \"seed\" in args_dict:\n",
    "        args[\"seed\"] = args_dict[\"seed\"]\n",
    "    else:\n",
    "        args[\"seed\"] = '0'\n",
    "\n",
    "    args[\"BS\"] = args_dict[\"batch_size\"]\n",
    "    args[\"lr\"] = args_dict[\"lr\"]\n",
    "    if \"weight_decay\" in args_dict:\n",
    "        args[\"weight_decay\"] = args_dict[\"weight_decay\"]\n",
    "    else:\n",
    "        args[\"weight_decay\"] = '0'\n",
    "    if \"lr_decay\" in args_dict:\n",
    "        args[\"lr_decay\"] = args_dict[\"lr_decay\"]\n",
    "    else:\n",
    "        args[\"lr_decay\"] = '0'\n",
    "    if \"p\" in args_dict:\n",
    "        args[\"p\"] = args_dict[\"p\"]\n",
    "    if \"precond\" in args_dict:\n",
    "        args[\"precond\"] = args_dict[\"precond\"]\n",
    "        args[\"beta2\"] = args_dict[\"beta2\"]\n",
    "        args[\"alpha\"] = args_dict[\"alpha\"]\n",
    "    else:\n",
    "        args[\"precond\"] = \"none\"\n",
    "        args[\"alpha\"] = \"none\"\n",
    "        args[\"beta2\"] = \"none\"\n",
    "\n",
    "    if args[\"optimizer\"] == \"Adam\":\n",
    "        args[\"beta1\"] = args_dict[\"beta1\"] if \"beta1\" in args_dict else '0.9'\n",
    "        args[\"beta2\"] = args_dict[\"beta2\"]\n",
    "        args[\"eps\"] = args_dict[\"eps\"] if \"eps\" in args_dict else '1e-8'\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_logs(logdir, dataset, optimizer, **filter_args):\n",
    "    \"\"\"\n",
    "    Return all logs in 'logdir' containing the filter hyperparams.\n",
    "    Dataset name should contain feature scaling, if any\n",
    "    e.g. 'dataset' or 'dataset(k_min,k_max)'.\n",
    "    \n",
    "    Returns the data in the log file and its arguments/hyperparams.\n",
    "    \"\"\"\n",
    "    global REMOVE_EMPTY_DATA\n",
    "    # Add\n",
    "    if \"corrupt\" in filter_args and filter_args['corrupt'] != \"none\":\n",
    "        # Add scale suffix to specify dataset    \n",
    "        dataset += filter_args['corrupt']\n",
    "    else:\n",
    "        # No setting specified, use wildcard to match all suffixes\n",
    "        dataset += \"*\"\n",
    "    # Find all files matching this pattern\n",
    "    for fname in glob.glob(f\"{logdir}/{dataset}/{optimizer}(*).pkl\"):\n",
    "        exp_args = unpack_args(fname)\n",
    "        # Skip if filter_args do not match args of this file\n",
    "        if not contain_dict(exp_args, filter_args):\n",
    "            continue\n",
    "        # Load data\n",
    "        data = loaddata(fname)\n",
    "        # Handle empty data files\n",
    "        if len(data) == 0:\n",
    "            print(fname, \"has no data!\")\n",
    "            if not REMOVE_EMPTY_DATA:\n",
    "                if \"y\" == input(\"Remove empty log files in the future without asking? (y/n)\"):\n",
    "                    print(\"Will remove without asking.\")\n",
    "                    REMOVE_EMPTY_DATA = True\n",
    "                else:\n",
    "                    print(\"Will ask again before removing.\")\n",
    "            else:\n",
    "                try:\n",
    "                    print(\"Removing\", fname)\n",
    "                    os.remove(fname)\n",
    "                except OSError as e:\n",
    "                    print (\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "            continue\n",
    "        # @XXX: hack to correct wrong initial ep>0 for L-SVRG\n",
    "        data = np.array(data)\n",
    "        ep0 = data[0,0]\n",
    "        if ep0 > 0.:\n",
    "            data[:,0] -= ep0\n",
    "        yield data, exp_args\n",
    "\n",
    "        \n",
    "# Gather data\n",
    "all_dfs = {}\n",
    "start_time = time.time()\n",
    "for exp in product(DATASETS, OPTIMIZERS):\n",
    "    exp_df = pd.DataFrame()\n",
    "    # Get all log data given the experiment and filter args\n",
    "    for data, args in get_logs(LOG_DIR, *exp, **FILTER_ARGS):\n",
    "        if ignore(args):\n",
    "            continue\n",
    "        # Get experiment log data\n",
    "        df = pd.DataFrame(data[:, :4], columns=LOG_COLS)\n",
    "        # Get args of interest\n",
    "        for col in ARG_COLS:\n",
    "            df[col] = args[col]\n",
    "        # Downsample by averaging metrics every AVG_DOWNSAMPLE epoch.\n",
    "        df[\"ep\"] = np.ceil(df[\"ep\"] / AVG_DOWNSAMPLE) * AVG_DOWNSAMPLE\n",
    "        df = df.groupby([\"ep\"] + ARG_COLS).mean().reset_index()\n",
    "        # Get data up to the prespecified epoch T\n",
    "        df = df[df[\"ep\"] <= T]\n",
    "        # @TODO: is this efficient?\n",
    "        exp_df = exp_df.append(df, ignore_index=True)\n",
    "    # Record all runs of exp in a single dataframe\n",
    "    all_dfs[exp] = exp_df\n",
    "\n",
    "    if len(exp_df) == 0:\n",
    "        print(\"No log data found for this experiment!\")\n",
    "        print(\"- Experiment:\", exp)\n",
    "        print(\"- filter_args:\", FILTER_ARGS)\n",
    "        continue\n",
    "data_gather_time = time.time() - start_time\n",
    "print(f\"Data frame lengths:\")\n",
    "for exp, df in all_dfs.items():\n",
    "    print(f\"{exp} -> {len(df)} data rows -> {len(df) // T} runs\")\n",
    "print(f\"Took about {data_gather_time:.2f} seconds to gather all these data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "81496a9e-0aa2-4969-a7b2-21081c7e3f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mnist', 'L-SVRG')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ep</th>\n",
       "      <th>lr</th>\n",
       "      <th>alpha</th>\n",
       "      <th>beta2</th>\n",
       "      <th>precond</th>\n",
       "      <th>loss</th>\n",
       "      <th>gradnorm</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.103515625e-05</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>2.303674</td>\n",
       "      <td>0.015701</td>\n",
       "      <td>0.899200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6.103515625e-05</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>2.297054</td>\n",
       "      <td>0.017216</td>\n",
       "      <td>0.890300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>6.103515625e-05</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>1.187880</td>\n",
       "      <td>1.103850</td>\n",
       "      <td>0.329450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>6.103515625e-05</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>0.453627</td>\n",
       "      <td>0.456129</td>\n",
       "      <td>0.125820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>6.103515625e-05</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.99</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>0.341007</td>\n",
       "      <td>0.254002</td>\n",
       "      <td>0.099100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5126</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>1e-07</td>\n",
       "      <td>0.99</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>5216.137327</td>\n",
       "      <td>0.921152</td>\n",
       "      <td>0.910800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5127</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>1e-07</td>\n",
       "      <td>0.99</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>3748.250835</td>\n",
       "      <td>0.418313</td>\n",
       "      <td>0.905743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5128</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>1e-07</td>\n",
       "      <td>0.99</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>2757.735963</td>\n",
       "      <td>0.365667</td>\n",
       "      <td>0.899000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5129</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>1e-07</td>\n",
       "      <td>0.99</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>1977.116441</td>\n",
       "      <td>0.241248</td>\n",
       "      <td>0.899911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5130</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>1e-07</td>\n",
       "      <td>0.99</td>\n",
       "      <td>hutchinson</td>\n",
       "      <td>1298.965695</td>\n",
       "      <td>0.641545</td>\n",
       "      <td>0.894820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5131 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ep               lr  alpha beta2     precond         loss  gradnorm  \\\n",
       "0      0.0  6.103515625e-05  0.001  0.99  hutchinson     2.303674  0.015701   \n",
       "1      2.0  6.103515625e-05  0.001  0.99  hutchinson     2.297054  0.017216   \n",
       "2      4.0  6.103515625e-05  0.001  0.99  hutchinson     1.187880  1.103850   \n",
       "3      6.0  6.103515625e-05  0.001  0.99  hutchinson     0.453627  0.456129   \n",
       "4      8.0  6.103515625e-05  0.001  0.99  hutchinson     0.341007  0.254002   \n",
       "...    ...              ...    ...   ...         ...          ...       ...   \n",
       "5126  16.0         0.015625  1e-07  0.99  hutchinson  5216.137327  0.921152   \n",
       "5127  18.0         0.015625  1e-07  0.99  hutchinson  3748.250835  0.418313   \n",
       "5128  20.0         0.015625  1e-07  0.99  hutchinson  2757.735963  0.365667   \n",
       "5129  22.0         0.015625  1e-07  0.99  hutchinson  1977.116441  0.241248   \n",
       "5130  24.0         0.015625  1e-07  0.99  hutchinson  1298.965695  0.641545   \n",
       "\n",
       "         error  \n",
       "0     0.899200  \n",
       "1     0.890300  \n",
       "2     0.329450  \n",
       "3     0.125820  \n",
       "4     0.099100  \n",
       "...        ...  \n",
       "5126  0.910800  \n",
       "5127  0.905743  \n",
       "5128  0.899000  \n",
       "5129  0.899911  \n",
       "5130  0.894820  \n",
       "\n",
       "[5131 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mnist', 'Adam')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ep</th>\n",
       "      <th>lr</th>\n",
       "      <th>alpha</th>\n",
       "      <th>beta2</th>\n",
       "      <th>precond</th>\n",
       "      <th>loss</th>\n",
       "      <th>gradnorm</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>none</td>\n",
       "      <td>0.99</td>\n",
       "      <td>none</td>\n",
       "      <td>2.303674</td>\n",
       "      <td>0.015701</td>\n",
       "      <td>0.89920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>none</td>\n",
       "      <td>0.99</td>\n",
       "      <td>none</td>\n",
       "      <td>0.137447</td>\n",
       "      <td>1.365845</td>\n",
       "      <td>0.04301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>none</td>\n",
       "      <td>0.99</td>\n",
       "      <td>none</td>\n",
       "      <td>0.117942</td>\n",
       "      <td>1.311841</td>\n",
       "      <td>0.03209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>none</td>\n",
       "      <td>0.99</td>\n",
       "      <td>none</td>\n",
       "      <td>0.128118</td>\n",
       "      <td>1.331316</td>\n",
       "      <td>0.03217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>none</td>\n",
       "      <td>0.99</td>\n",
       "      <td>none</td>\n",
       "      <td>0.124661</td>\n",
       "      <td>1.147403</td>\n",
       "      <td>0.02900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0009765625</td>\n",
       "      <td>none</td>\n",
       "      <td>0.99</td>\n",
       "      <td>none</td>\n",
       "      <td>0.037298</td>\n",
       "      <td>0.528722</td>\n",
       "      <td>0.00934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0009765625</td>\n",
       "      <td>none</td>\n",
       "      <td>0.99</td>\n",
       "      <td>none</td>\n",
       "      <td>0.036511</td>\n",
       "      <td>0.123733</td>\n",
       "      <td>0.00888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0009765625</td>\n",
       "      <td>none</td>\n",
       "      <td>0.99</td>\n",
       "      <td>none</td>\n",
       "      <td>0.047172</td>\n",
       "      <td>1.565055</td>\n",
       "      <td>0.00989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0009765625</td>\n",
       "      <td>none</td>\n",
       "      <td>0.99</td>\n",
       "      <td>none</td>\n",
       "      <td>0.042677</td>\n",
       "      <td>0.567044</td>\n",
       "      <td>0.00926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0009765625</td>\n",
       "      <td>none</td>\n",
       "      <td>0.99</td>\n",
       "      <td>none</td>\n",
       "      <td>0.040352</td>\n",
       "      <td>0.096161</td>\n",
       "      <td>0.00907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1170 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ep            lr alpha beta2 precond      loss  gradnorm    error\n",
       "0      0.0      0.015625  none  0.99    none  2.303674  0.015701  0.89920\n",
       "1      2.0      0.015625  none  0.99    none  0.137447  1.365845  0.04301\n",
       "2      4.0      0.015625  none  0.99    none  0.117942  1.311841  0.03209\n",
       "3      6.0      0.015625  none  0.99    none  0.128118  1.331316  0.03217\n",
       "4      8.0      0.015625  none  0.99    none  0.124661  1.147403  0.02900\n",
       "...    ...           ...   ...   ...     ...       ...       ...      ...\n",
       "1165  16.0  0.0009765625  none  0.99    none  0.037298  0.528722  0.00934\n",
       "1166  18.0  0.0009765625  none  0.99    none  0.036511  0.123733  0.00888\n",
       "1167  20.0  0.0009765625  none  0.99    none  0.047172  1.565055  0.00989\n",
       "1168  22.0  0.0009765625  none  0.99    none  0.042677  0.567044  0.00926\n",
       "1169  24.0  0.0009765625  none  0.99    none  0.040352  0.096161  0.00907\n",
       "\n",
       "[1170 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, (exp, df) in enumerate(all_dfs.items()):\n",
    "    if i == 3: break\n",
    "    print(exp)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd98f3f7-60e4-403b-92b8-795d8067e530",
   "metadata": {},
   "source": [
    "## Get best hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca59c937-0832-4f7f-a48e-2e39b74d05a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best hyperparams for ('mnist', 'L-SVRG')\n",
      "Finding best hyperparams for ('mnist', 'Adam')\n"
     ]
    }
   ],
   "source": [
    "preconds = [] if \"precond\" not in ARG_COLS else [\"none\", \"hutchinson\"]\n",
    "for exp, df in all_dfs.items():\n",
    "    if exp[1] == \"Adam\": continue\n",
    "    alphas = set([] if \"alpha\" not in ARG_COLS else df[\"alpha\"])\n",
    "    betas = set([] if \"beta2\" not in ARG_COLS else df[\"beta2\"])\n",
    "    lrs = set([] if \"lr\" not in ARG_COLS else df[\"lr\"])\n",
    "    break\n",
    "\n",
    "best_dfs = {}\n",
    "best_dfs_alpha = {}\n",
    "best_dfs_beta = {}\n",
    "best_dfs_lr = {}\n",
    "best_dfs_precond = {}\n",
    "for exp in product(DATASETS, OPTIMIZERS):\n",
    "    print(\"Finding best hyperparams for\", exp)\n",
    "    best_dfs_alpha[exp] = {}\n",
    "    best_dfs_beta[exp] = {}\n",
    "    best_dfs_lr[exp] = {}\n",
    "    best_dfs_precond[exp] = {}\n",
    "    # Get last metrics/performance (supposed to be epoch-smoothed for better results)\n",
    "    exp_df = all_dfs[exp]\n",
    "    max_ep = exp_df.groupby(ARG_COLS, sort=False)[\"ep\"].transform(max)\n",
    "    perf = exp_df[exp_df[\"ep\"] == max_ep].drop(\"ep\", axis=1)\n",
    "    # Find the minimum aggregate metric (based on mean, median, etc.)\n",
    "    def find_best_hyperparams(perf):\n",
    "        if AGG == \"mean\":\n",
    "            agg_perf = perf.groupby(ARG_COLS).mean()\n",
    "        elif AGG == \"median\":\n",
    "            agg_perf = perf.groupby(ARG_COLS).median()\n",
    "        # Get the aggregated perf that minimizes the chosen metric\n",
    "        min_agg_perf = agg_perf[agg_perf[METRIC] == agg_perf.min()[METRIC]]\n",
    "        return min_agg_perf.index\n",
    "    # Get the data associated with the args of the min aggregated metric\n",
    "    exp_df = exp_df.set_index(ARG_COLS)\n",
    "    best_dfs[exp] = exp_df.loc[find_best_hyperparams(perf)]\n",
    "    for alpha in alphas:\n",
    "        best_dfs_alpha[exp][alpha] = exp_df.loc[find_best_hyperparams(perf[perf[\"alpha\"] == alpha])]\n",
    "    for beta in betas:\n",
    "        best_dfs_beta[exp][beta] = exp_df.loc[find_best_hyperparams(perf[perf[\"beta2\"] == beta])]\n",
    "    for lr in lrs:\n",
    "        best_dfs_lr[exp][lr] = exp_df.loc[find_best_hyperparams(perf[perf[\"lr\"] == lr])]\n",
    "    for precond in preconds:\n",
    "        best_dfs_precond[exp][precond] = exp_df.loc[find_best_hyperparams(perf[perf[\"precond\"] == precond])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36d9874c-6a23-45c9-80f0-67bebd27b0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparams for each optimizer on each dataset given the following setting:\n",
      "{}\n",
      "\n",
      "('mnist', 'L-SVRG')\n",
      "- lr = 2**-6\n",
      "- alpha = 0.1\n",
      "- beta2 = 0.99\n",
      "- precond = hutchinson\n",
      "\n",
      "('mnist', 'Adam')\n",
      "- lr = 2**-10\n",
      "- alpha = none\n",
      "- beta2 = 0.99\n",
      "- precond = none\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparams for each optimizer on each dataset given the following setting:\")\n",
    "print(FILTER_ARGS)\n",
    "print()\n",
    "for exp, df in best_dfs.items():\n",
    "    print(exp)\n",
    "    for arg, val in zip(ARG_COLS, df.index[0]):\n",
    "        if arg == \"lr\":\n",
    "            val = \"2**\" + str(int(log2(float(val))))\n",
    "        print(f\"- {arg} = {val}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589bb07b",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "161b091b-9c0a-48e0-9f37-7ab51ade0273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types\n",
      "ep float64\n",
      "loss float64\n",
      "gradnorm float64\n",
      "error float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Types\")\n",
    "for col in df.columns:\n",
    "    print(col, df[col].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b7f87599-2e6b-4030-b71e-2455347d9705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rates:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'2**-10', '2**-12', '2**-14', '2**-6', '2**-8'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Learning rates:\")\n",
    "for exp, df in all_dfs.items():\n",
    "    display(set(\"2**\"+str(int(log2(float(lr)))) for lr in df[\"lr\"]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "329af229-7514-43cf-a3b8-013508e623f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range\n",
      "ep: (0.0, 24.0)\n",
      "loss: (0.02400844679877423, inf)\n",
      "gradnorm: (3.2354813204923026e-05, inf)\n",
      "error: (0.0071, 0.91315)\n"
     ]
    }
   ],
   "source": [
    "print(\"Range\")\n",
    "for col in df.columns:\n",
    "    if df[col].dtypes != \"object\":\n",
    "        print(f\"{col}: ({df[col].min():}, {df[col].max()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d2b7628b-1e89-482a-9cdc-055e14187188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: (0.0, 24.0)\n",
      "loss: (0.02400844679877423, 9562093005.43324)\n",
      "gradnorm: (3.2354813204923026e-05, 8879064985.6)\n",
      "error: (0.0071, 0.91315)\n"
     ]
    }
   ],
   "source": [
    "# Set inf values to nan and recheck range\n",
    "VERYBIGNUMBER = 10**10\n",
    "df[df == float(\"inf\")] = np.nan\n",
    "df[df[[\"loss\",\"gradnorm\"]] > VERYBIGNUMBER] = np.nan\n",
    "for col in df.columns:\n",
    "    if df[col].dtypes != \"object\":\n",
    "        print(f\"{col}: ({df[col].min():}, {df[col].max()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "302e0f60-db61-4ed7-bf7f-7f38f554a0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for NaNs in each column for each df.\n",
      "ep\n",
      "- ('mnist', 'L-SVRG'): 0\n",
      "- ('mnist', 'Adam'): 0\n",
      "lr\n",
      "- ('mnist', 'L-SVRG'): 0\n",
      "- ('mnist', 'Adam'): 0\n",
      "alpha\n",
      "- ('mnist', 'L-SVRG'): 0\n",
      "- ('mnist', 'Adam'): 0\n",
      "beta2\n",
      "- ('mnist', 'L-SVRG'): 0\n",
      "- ('mnist', 'Adam'): 0\n",
      "precond\n",
      "- ('mnist', 'L-SVRG'): 0\n",
      "- ('mnist', 'Adam'): 0\n",
      "loss\n",
      "- ('mnist', 'L-SVRG'): 307\n",
      "- ('mnist', 'Adam'): 0\n",
      "gradnorm\n",
      "- ('mnist', 'L-SVRG'): 476\n",
      "- ('mnist', 'Adam'): 0\n",
      "error\n",
      "- ('mnist', 'L-SVRG'): 0\n",
      "- ('mnist', 'Adam'): 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Check for NaNs in each column for each df.\")\n",
    "for col in df.columns:\n",
    "    print(col)\n",
    "    for exp, df in all_dfs.items():\n",
    "        print(f\"- {exp}: {df[col].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6130b8ea",
   "metadata": {},
   "source": [
    "## Plot best performance of each optimizer on each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3e55552a-4663-445c-b633-d6124f3b5bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('legend', fontsize=LEGEND_FONTSIZE, loc=LEGEND_LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "af103b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting lines for ('mnist', 'L-SVRG')...\n",
      "Plotting lines for ('mnist', 'Adam')...\n",
      "Took about 1.40 seconds to create this plot.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Plot 3 rows each one showing some performance metric,\n",
    "# where the columns are the dataset on which the optim is run.\n",
    "fig, axes = plt.subplots(1, 3)\n",
    "fig.set_size_inches(ASPECT * HEIGHT * 3, HEIGHT * 1)\n",
    "plt.suptitle(rf\"Best performances on MNIST\")\n",
    "for optimizer in OPTIMIZERS:\n",
    "    exp = (dataset, optimizer)\n",
    "    if exp not in best_dfs:\n",
    "        continue\n",
    "    # Get hyperparams of best performance of 'optimizer' on 'dataset'\n",
    "    args = {k:v for k,v in zip(best_dfs[exp].index.names, best_dfs[exp].index[0])}\n",
    "    exp_df = best_dfs[exp].reset_index()\n",
    "    # Show power of lr as 2^lr_pow\n",
    "    lr_pow = round(log2(float(args['lr'])))\n",
    "    if optimizer == \"Adam\":\n",
    "        sublabel = rf\"$\\eta = 2^{{{lr_pow}}}$, $\\beta_1=0.9$, $\\beta_2={args['beta2']}$\"\n",
    "    else:\n",
    "        sublabel = rf\"$\\eta = 2^{{{lr_pow}}}$, $\\alpha={args['alpha']}$, $\\beta={args['beta2']}$\"\n",
    "    label = rf\"{optimizer}({sublabel})\"\n",
    "    print(f\"Plotting lines for {exp}...\")\n",
    "    sns.lineplot(x=\"ep\", y=\"loss\", label=label, ax=axes[0], data=exp_df)\n",
    "    sns.lineplot(x=\"ep\", y=\"gradnorm\", label=label, ax=axes[1], data=exp_df)\n",
    "    sns.lineplot(x=\"ep\", y=\"error\", label=label, ax=axes[2], data=exp_df)\n",
    "# Loss\n",
    "axes[0].set_ylabel(r\"$P(w_t)$\")\n",
    "axes[0].set_xlabel(\"Effective Passes\")\n",
    "axes[0].legend()\n",
    "# Gradnorm\n",
    "axes[1].set(yscale=\"log\")\n",
    "axes[1].set_ylabel(r\"$||\\nabla P(w_t)||^2$\")\n",
    "axes[1].set_xlabel(\"Effective Passes\")\n",
    "axes[1].legend()\n",
    "# Error\n",
    "axes[2].set(yscale=\"log\")\n",
    "axes[2].set_ylabel(\"Error\")\n",
    "axes[2].set_xlabel(\"Effective Passes\")\n",
    "axes[2].legend()\n",
    "fig.tight_layout()\n",
    "\n",
    "# Create a string out of filter args and save figure\n",
    "plt.savefig(f\"plots/perf({SETTINGS_STR}).pdf\")\n",
    "plt.close()\n",
    "plot_best_time = time.time() - start_time\n",
    "print(f\"Took about {plot_best_time:.2f} seconds to create this plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737fe2bc",
   "metadata": {},
   "source": [
    "## Plot best performance given a fixed value of either $\\alpha$, $\\beta$, or $\\eta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1f03a7d0-98c5-484b-93ed-9953367298e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_greek = {\n",
    "    \"loss\": r\"$P(w_t)$\",\n",
    "    \"gradnorm\": r\"$||\\nabla P(w_t)||^2$\",\n",
    "    \"error\": \"error\"\n",
    "}\n",
    "\n",
    "best_dfs_mode= {\n",
    "    \"alphas\": best_dfs_alpha,\n",
    "    \"betas\": best_dfs_beta,\n",
    "    \"lrs\": best_dfs_lr,\n",
    "    \"precond\": best_dfs_precond,\n",
    "}\n",
    "mode_greek = {\n",
    "    \"alphas\": r\"$\\alpha$\",\n",
    "    \"betas\": r\"$\\beta$\",\n",
    "    \"lrs\": r\"$\\eta$\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "34ec86ec-b50c-4783-bef8-0ef9eef17caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting lines for ('mnist', 'L-SVRG')...\n",
      "Plotting lines for ('mnist', 'Adam')...\n",
      "Took about 1.19 seconds to create this plot.\n",
      "Plotting lines for ('mnist', 'L-SVRG')...\n",
      "Plotting lines for ('mnist', 'Adam')...\n",
      "Took about 2.18 seconds to create this plot.\n",
      "Plotting lines for ('mnist', 'L-SVRG')...\n",
      "Plotting lines for ('mnist', 'Adam')...\n",
      "Took about 1.17 seconds to create this plot.\n",
      "Plotting lines for ('mnist', 'L-SVRG')...\n",
      "Plotting lines for ('mnist', 'Adam')...\n",
      "Took about 2.16 seconds to create this plot.\n",
      "CPU times: user 6.61 s, sys: 87.4 ms, total: 6.7 s\n",
      "Wall time: 6.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for y in (\"error\", \"gradnorm\"):\n",
    "    for mode in (\"betas\", \"lrs\"):\n",
    "        valid_optimizers = [opt for opt in OPTIMIZERS if not (mode == \"alphas\" and opt == \"Adam\")]\n",
    "\n",
    "        start_time = time.time()\n",
    "        # Plot data for all optim, datasets, and args\n",
    "        fig, axes = plt.subplots(1, len(valid_optimizers))\n",
    "        fig.set_size_inches(ASPECT * HEIGHT_LARGE * len(valid_optimizers), HEIGHT_LARGE * 1)\n",
    "        title = rf\"Best {y_greek[y]} given {mode_greek[mode]}\"\n",
    "        plt.suptitle(title)\n",
    "        for i, optimizer in enumerate(valid_optimizers):\n",
    "            exp = (dataset, optimizer)\n",
    "            if exp not in best_dfs_mode[mode]:\n",
    "                continue\n",
    "            exp_df = pd.concat(best_dfs_mode[mode][exp].values()).reset_index()\n",
    "            if len(exp_df) == 0:\n",
    "                continue\n",
    "            exp_df[\"lr\"] = exp_df[\"lr\"].astype(float)\n",
    "            exp_df[\"alpha\"] = exp_df[\"alpha\"].astype(str)\n",
    "            exp_df[\"beta2\"] = exp_df[\"beta2\"].astype(str)\n",
    "            print(f\"Plotting lines for {exp}...\")\n",
    "            if mode == \"lrs\":\n",
    "                exp_df = exp_df.sort_values(\"alpha\", ascending=False)  # none is thinest\n",
    "                exp_df = exp_df.sort_values(\"beta2\", ascending=False)  # none is solid, avg is dashed, etc.\n",
    "                sns.lineplot(ax=axes[i], x=\"ep\", y=y,\n",
    "                             hue=\"lr\", hue_norm=LogNorm(), palette=\"vlag\",\n",
    "                             size=\"alpha\", style=\"beta2\", data=exp_df)\n",
    "            elif mode == \"betas\":\n",
    "                exp_df = exp_df.sort_values(\"alpha\", ascending=True)  # none is blue, etc.\n",
    "                exp_df = exp_df.sort_values(\"beta2\", ascending=True)  # nums first, to be consistent with Adam\n",
    "                sns.lineplot(ax=axes[i], x=\"ep\", y=y,\n",
    "                             hue=\"beta2\", size=\"lr\", size_norm=LogNorm(), style=\"alpha\", data=exp_df)\n",
    "            elif mode == \"alphas\":\n",
    "                exp_df = exp_df.sort_values(\"alpha\", ascending=True)  # none is blue, etc.\n",
    "                exp_df = exp_df.sort_values(\"beta2\", ascending=False)  # none is solid, avg is dashed, etc.\n",
    "                sns.lineplot(ax=axes[i], x=\"ep\", y=y,\n",
    "                             hue=\"alpha\", size=\"lr\", size_norm=LogNorm(), style=\"beta2\", data=exp_df)\n",
    "            axes[i].set(yscale=\"log\")\n",
    "            axes[i].set_title(rf\"$\\tt {optimizer}({dataset})$\")\n",
    "            axes[i].set_ylabel(rf\"{y_greek[y]}\")\n",
    "            axes[i].set_xlabel(rf\"Effective Passes\")\n",
    "        fig.tight_layout()\n",
    "\n",
    "        # Create a string out of filter args and save figure\n",
    "        plt.savefig(f\"plots/{y}_given_{mode}({SETTINGS_STR}).pdf\")\n",
    "        plt.close()\n",
    "        plot_time = time.time() - start_time\n",
    "        print(f\"Took about {plot_time:.2f} seconds to create this plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52782ee8-b057-41d3-800f-b18b55402aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba240ec-3f7e-4b7c-93fc-f56e8b8eed07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
